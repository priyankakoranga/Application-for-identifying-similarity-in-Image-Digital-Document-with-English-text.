{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "stopwords_en = stopwords.words(\"english\")\n",
    "import import_ipynb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way to calculate similarities\n",
    "def similarity_check():\n",
    "    \n",
    "    #data preprocessing\n",
    "    def preprocessing(raw):\n",
    "        #tokenizing the document\n",
    "        wordlist = nltk.word_tokenize(raw) \n",
    "        #transforming each word to lower case and removing stopwords\n",
    "        text = [w.lower() for w in wordlist if w not in stopwords_en]\n",
    "        return text\n",
    "    \n",
    "    #preprocessing the first document\n",
    "    f1 = open('result_text1.txt','r',encoding = \"utf8\")\n",
    "    text1 = preprocessing(f1.read())\n",
    "\n",
    "    #preprocessing the second document\n",
    "    f2 = open('result_text2.txt','r',encoding = \"utf8\")\n",
    "    text2 = preprocessing(f2.read())\n",
    "    \n",
    "    #tf-idf vectorizer\n",
    "    \n",
    "    word_set = set(text1).union(set(text2))\n",
    "\n",
    "    #step 1: calculate TF\n",
    "    freqd_text1 = FreqDist(text1)  \n",
    "    text1_length = len(text1)\n",
    "    text1_tf_dict = dict.fromkeys(word_set,0)\n",
    "    #to store word-count pairs of all the words from common word_set along with their individual count\n",
    "    for word in text1:\n",
    "        text1_tf_dict[word] = freqd_text1[word]/text1_length\n",
    "        \n",
    "    #calculate term frequency for 2nd text\n",
    "    freqd_text2 = FreqDist(text2)\n",
    "    text2_length = len(text2)\n",
    "    text2_tf_dict = dict.fromkeys(word_set,0)\n",
    "    #to store word-count pairs of all the words from common word_set along with their individual count\n",
    "    for word in text2:\n",
    "        text2_tf_dict[word] = freqd_text2[word]/text2_length\n",
    "\n",
    "    #step 2: calculating IDF \n",
    "    text12_idf_dict = dict.fromkeys(word_set,0)\n",
    "    text12_length = 2\n",
    "    for word in text12_idf_dict.keys():\n",
    "        if word in text1:\n",
    "            text12_idf_dict[word]+=1\n",
    "        if word in text2:\n",
    "            text12_idf_dict[word]+=1\n",
    "\n",
    "    \n",
    "    for word,val in text12_idf_dict.items():\n",
    "        text12_idf_dict[word] = 1+math.log(text12_length/(float(val)))\n",
    "    \n",
    "    #calculating TF-IDF\n",
    "    text1_tfidf_dict = dict.fromkeys(word_set,0)\n",
    "    for word in text1:\n",
    "        text1_tfidf_dict[word] =(text1_tf_dict[word])*(text12_idf_dict[word])\n",
    "\n",
    "    text2_tfidf_dict = dict.fromkeys(word_set,0)\n",
    "    for word in text2:\n",
    "        text2_tfidf_dict[word] = (text2_tf_dict[word])*(text12_idf_dict[word])\n",
    "\n",
    "    #finding the similarity- using cosine similarity\n",
    "    v1 = list(text1_tfidf_dict.values())\n",
    "    v2 = list(text2_tfidf_dict.values())\n",
    "    similarity1 = 1-nltk.cluster.cosine_distance(v1,v2)\n",
    "    print(\"similarity index :{:4.2f} %\".format(similarity1*100))\n",
    "    \n",
    "\n",
    "    #Similarity computation using doc2vec \n",
    "    from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "    taggeddocs = []\n",
    "    doc1 = TaggedDocument(words = text1,tags = [u'file1'])\n",
    "    taggeddocs.append(doc1)\n",
    "    doc2 = TaggedDocument(words = text2,tags = [u'file2'])\n",
    "    taggeddocs.append(doc2)\n",
    "\n",
    "    #build the model\n",
    "    model = Doc2Vec(taggeddocs,dm=0,alpha=0.025,size=20,min_aclpha = 0.025,min_count=0)\n",
    "\n",
    "\n",
    "    #training\n",
    "    for epoch in range(80):\n",
    "        if epoch%20 ==0:\n",
    "            print('Now training epoch % epoch')\n",
    "        model.train(taggeddocs,total_examples=model.corpus_count,epochs=model.iter)\n",
    "        model.alpha -=0.002\n",
    "        model.min_alpha = model.alpha\n",
    "\n",
    "    similarity = model.n_similarity(text1,text2)\n",
    "    print(similarity)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity index :80.72 %\n",
      "Now training epoch % epoch\n",
      "Now training epoch % epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRIYANKA\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:85: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training epoch % epoch\n",
      "Now training epoch % epoch\n",
      "0.8213873\n",
      "0.8213873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRIYANKA\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: DeprecationWarning: Call to deprecated `n_similarity` (Method will be removed in 4.0.0, use self.wv.n_similarity() instead).\n"
     ]
    }
   ],
   "source": [
    "print(similarity_check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_set = set(text1).union(set(text2))\n",
    "\n",
    "freqd_text1 = FreqDist(text1)\n",
    "\n",
    "text1_count_dict = dict.fromkeys(word_set,0)\n",
    "for word in text1:\n",
    "    text1_count_dict[word] = freqd_text1[word]\n",
    "    \n",
    "\n",
    "freqd_text2 = FreqDist(text2)\n",
    "text2_count_dict = dict.fromkeys(word_set,0)\n",
    "for word in text2:\n",
    "    text2_count_dict[word] = freqd_text2[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "freqd_text1 = FreqDist(text1)\n",
    "text1_length = len(text1)\n",
    "text1_tf_dict = dict.fromkeys(word_set,0)\n",
    "for word in text1:\n",
    "    text1_tf_dict[word] = freqd_text1[word]/text1_length\n",
    "    \n",
    "freqd_text2 = FreqDist(text2)\n",
    "text2_length = len(text2)\n",
    "text2_tf_dict = dict.fromkeys(word_set,0)\n",
    "for word in text2:\n",
    "    text2_tf_dict[word] = freqd_text2[word]/text2_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text12_idf_dict = dict.fromkeys(word_set,0)\n",
    "text12_length = 2\n",
    "for word in text12_idf_dict.keys():\n",
    "    if word in text1:\n",
    "        text12_idf_dict[word]+=1\n",
    "    if word in text2:\n",
    "        text12_idf_dict[word]+=1\n",
    "\n",
    "import math\n",
    "for word,val in text12_idf_dict.items():\n",
    "    text12_idf_dict[word] = 1+math.log(text12_length/(float(val)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text1_tfidf_dict = dict.fromkeys(word_set,0)\n",
    "for word in text1:\n",
    "    text1_tfidf_dict[word] =(text1_tf_dict[word])*(text12_idf_dict[word])\n",
    "    \n",
    "text2_tfidf_dict = dict.fromkeys(word_set,0)\n",
    "for word in text2:\n",
    "    text2_tfidf_dict[word] = (text2_tf_dict[word])*(text12_idf_dict[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v1 = list(text1_tfidf_dict.values())\n",
    "v2 = list(text2_tfidf_dict.values())\n",
    "similarity = 1-nltk.cluster.cosine_distance(v1,v2)\n",
    "print(\"similarity index :{:4.2f} %\".format(similarity*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "taggeddocs = []\n",
    "doc1 = TaggedDocument(words = text1,tags = [u'file1'])\n",
    "taggeddocs.append(doc1)\n",
    "doc2 = TaggedDocument(words = text2,tags = [u'file2'])\n",
    "taggeddocs.append(doc2)\n",
    "\n",
    "#build the model\n",
    "model = Doc2Vec(taggeddocs,dm=0,alpha=0.025,size=20,min_alpha = 0.025,min_count=0)\n",
    "\n",
    "\n",
    "#training\n",
    "for epoch in range(80):\n",
    "    if epoch%20 ==0:\n",
    "        print('Now training epoch % epoch')\n",
    "    model.train(taggeddocs,total_examples=model.corpus_count,epochs=model.iter)\n",
    "    model.alpha -=0.002\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarity = model.n_similarity(text1,text2)\n",
    "print(\"similarity index : {:4.2f} %\".format(similarity*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
