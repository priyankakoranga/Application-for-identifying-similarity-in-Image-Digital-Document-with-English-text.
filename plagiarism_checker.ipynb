{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check plagiarism of two files\n",
    "\n",
    "def plagiarism_check(n):\n",
    "    \n",
    "    #document whose plagiarism is to be checked\n",
    "    with open('result_text1.txt','r',encoding='utf-8') as file1:\n",
    "        text1 = file1.read()\n",
    "        \n",
    "        #converting the text of the file to lower case and tokenizing the first file into sentences\n",
    "        text1 = text1.lower()\n",
    "        s_list = nltk.sent_tokenize(text1)\n",
    "\n",
    "    sentences = []\n",
    "    #removing the special characters from the text for better results.\n",
    "    for sents in s_list:\n",
    "        x = re.sub(r'[^a-zA-Z0-9]+',\" \", sents)\n",
    "        sentences.append(x)\n",
    "    \n",
    "    #function to find the n-gram (value of n is given by the user)\n",
    "    def get_ngrams(tokenized,n):\n",
    "        ngrams = []\n",
    "        for i in range(len(tokenized)-(n-1)):\n",
    "            ngrams.append(tokenized[i:i+n])\n",
    "        return ngrams\n",
    "    \n",
    "    #storing ngrams of each sentence in a list \n",
    "    ngram_list = []\n",
    "    for i in sentences:\n",
    "        tokenized = i.split()\n",
    "        ng = get_ngrams(tokenized,n)\n",
    "        ngram_list.append(ng)\n",
    "\n",
    "    #original document\n",
    "    with open('result_text2.txt','r',encoding='utf-8') as file2:\n",
    "        text2 = file2.read()\n",
    "        text2 = text2.lower()\n",
    "\n",
    "    org_text = re.sub(r'[^a-zA-Z0-9]+',\" \", text2)\n",
    "\n",
    "    tokenize_org = org_text.split()\n",
    "\n",
    "    #entire file is tokenized and stored in a list (unlike 1st file is tokenized sentence wise)\n",
    "    ngram_org = []\n",
    "    ngram_org = get_ngrams(tokenize_org,n)\n",
    "\n",
    "    #checking how much percentage of each sentence is plagrised and storing the % per sentence in a list\n",
    "    plag_list = []\n",
    "    for i in ngram_list:\n",
    "        count = 0\n",
    "        for j in i:\n",
    "            if j in ngram_org:\n",
    "                count=count+1\n",
    "        p = (count/len(i))  * 100\n",
    "        plag_list.append(p)\n",
    "\n",
    "    #finding the average plagriasm of all the sentences\n",
    "    sum1=0\n",
    "    for i in plag_list:\n",
    "        sum1 = sum1+i\n",
    "    avg = sum1/len(plag_list)\n",
    "    print(sum1,plag_list)\n",
    "    print(avg)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check plagiarism of two files\n",
    "\n",
    "def plagiarism_check(n):\n",
    "    \n",
    "    #document whose plagiarism is to be checked\n",
    "    with open('result_text1.txt','r',encoding='utf-8') as file1:\n",
    "        text1 = file1.read()\n",
    "        \n",
    "        #converting the text of the file to lower case and tokenizing the first file into sentences\n",
    "        text1 = text1.lower()\n",
    "        s_list = nltk.sent_tokenize(text1)\n",
    "\n",
    "    sentences = []\n",
    "    #removing the special characters from the text for better results.\n",
    "    for sents in s_list:\n",
    "        x = re.sub(r'[^a-zA-Z0-9]+',\" \", sents)\n",
    "        x = re.sub(' +',' ',x)\n",
    "        sentences.append(x) \n",
    "    \n",
    "    #function to find the n-gram (value of n is given by the user)\n",
    "    def get_ngrams(tokenized,n):\n",
    "        ngrams = []\n",
    "        for i in range(len(tokenized)-(n-1)):\n",
    "            ngrams.append(tokenized[i:i+n])\n",
    "        return ngrams\n",
    "    \n",
    "    #storing ngrams of each sentence in a list \n",
    "    ngram_list = []\n",
    "    for i in sentences:\n",
    "        tokenized = i.split()\n",
    "        print\n",
    "        ng = get_ngrams(tokenized,n)\n",
    "        ngram_list.append(ng)\n",
    "\n",
    "    #original document\n",
    "    with open('result_text2.txt','r',encoding='utf-8') as file2:\n",
    "        text2 = file2.read()\n",
    "        text2 = text2.lower()\n",
    "\n",
    "    org_text = re.sub(r'[^a-zA-Z0-9]+',\" \", text2)\n",
    "\n",
    "    tokenize_org = org_text.split()\n",
    "\n",
    "    #entire file is tokenized and stored in a list (unlike 1st file is tokenized sentence wise)\n",
    "    ngram_org = []\n",
    "    ngram_org = get_ngrams(tokenize_org,n)\n",
    "\n",
    "    #checking how much percentage of each sentence is plagrised and storing the % per sentence in a list\n",
    "    plag_list = []\n",
    "    for i in ngram_list:\n",
    "        print(i)\n",
    "        count = 0\n",
    "        for j in i:\n",
    "            if j in ngram_org:\n",
    "                count=count+1\n",
    "        p = (count/len(i))  * 100\n",
    "        plag_list.append(p)\n",
    "\n",
    "    #finding the average plagriasm of all the sentences\n",
    "    sum1=0\n",
    "    print(plag_list)\n",
    "    for i in plag_list:\n",
    "        sum1 = sum1+i\n",
    "    avg = sum1/len(plag_list)\n",
    "    print(sum1,plag_list)\n",
    "    print(avg)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'use', 'of'], ['use', 'of', 'scientific']]\n",
      "[['knowledge', 'for', 'practical'], ['for', 'practical', 'purposes'], ['practical', 'purposes', 'or'], ['purposes', 'or', '174'], ['or', '174', 'applications'], ['174', 'applications', 'whether'], ['applications', 'whether', 'inindustry'], ['whether', 'inindustry', 'orin'], ['inindustry', 'orin', 'our'], ['orin', 'our', 'vv'], ['our', 'vv', 'everyday'], ['vv', 'everyday', 'tives']]\n",
      "[50.0, 0.0]\n",
      "50.0 [50.0, 0.0]\n",
      "25.0\n",
      "25.0\n"
     ]
    }
   ],
   "source": [
    "print(plagiarism_check(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
